{
    "nbformat": 4,
    "nbformat_minor": 0,
    "metadata": {
        "colab": {
            "provenance": [],
            "gpuType": "T4",
            "accelerator": "GPU"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3"
        },
        "language_info": {
            "name": "python"
        },
        "accelerator": "GPU"
    },
    "cells": [
        {
            "cell_type": "markdown",
            "source": [
                "# üöÄ Free GPU LLM Host (Ollama + Ngrok)\n",
                "Run this notebook to host `llama3.1:8b` (Q4 quantized) on a Google T4 GPU and expose it via API to your Render backend.\n",
                "\n",
                "**IMPORTANT**: Make sure to select **Runtime** ‚Üí **Change runtime type** ‚Üí **T4 GPU** before running!"
            ],
            "metadata": {
                "id": "header"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# 0. Verify GPU is Available\n",
                "!nvidia-smi\n",
                "import torch\n",
                "print(f\"\\n‚úÖ CUDA Available: {torch.cuda.is_available()}\")\n",
                "if torch.cuda.is_available():\n",
                "    print(f\"‚úÖ GPU Device: {torch.cuda.get_device_name(0)}\")\n",
                "else:\n",
                "    print(\"‚ö†Ô∏è WARNING: No GPU detected! Go to Runtime ‚Üí Change runtime type ‚Üí T4 GPU\")"
            ],
            "metadata": {
                "id": "check_gpu"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# 1. Install Ollama & Ngrok\n",
                "!curl -fsSL https://ollama.com/install.sh | sh\n",
                "!pip install pyngrok"
            ],
            "metadata": {
                "id": "install"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# 2. Configure Ngrok (REQUIRED)\n",
                "import getpass\n",
                "from pyngrok import ngrok, conf\n",
                "\n",
                "print(\"Enter your Ngrok Authtoken (from dashboard.ngrok.com):\")\n",
                "token = getpass.getpass()\n",
                "conf.get_default().auth_token = token"
            ],
            "metadata": {
                "id": "auth"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# 3. Start Ollama Server with GPU Support\n",
                "import os\n",
                "import threading\n",
                "import time\n",
                "import subprocess\n",
                "\n",
                "def start_ollama():\n",
                "    # Configure Ollama environment\n",
                "    os.environ['OLLAMA_HOST'] = '0.0.0.0:11434'\n",
                "    os.environ['OLLAMA_ORIGINS'] = '*'\n",
                "    # Enable GPU by default (Ollama auto-detects CUDA)\n",
                "    os.environ['OLLAMA_GPU_LAYERS'] = '999'  # Use all GPU layers\n",
                "    subprocess.run([\"ollama\", \"serve\"])\n",
                "\n",
                "threading.Thread(target=start_ollama, daemon=True).start()\n",
                "print(\"‚è≥ Starting Ollama server...\")\n",
                "time.sleep(8)\n",
                "\n",
                "# Verify server is running\n",
                "!curl -s http://localhost:11434 && echo \"‚úÖ Ollama server is UP\" || echo \"‚ùå Server failed to start\"\n",
                "\n",
                "# Pull Model (should automatically use GPU)\n",
                "print(\"\\n‚¨áÔ∏è Pulling llama3.1:8b model (Q4 quantized, optimized for T4)...\")\n",
                "!ollama pull llama3.1:8b\n",
                "\n",
                "# Test inference to confirm GPU usage\n",
                "print(\"\\nüß™ Testing GPU inference...\")\n",
                "!ollama run llama3.1:8b \"Say hi in 3 words\" --verbose"
            ],
            "metadata": {
                "id": "start"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# 4. Expose Public URL\n",
                "public_url = ngrok.connect(11434, bind_tls=True).public_url\n",
                "print(\"‚úÖ Universal LLM API Ready!\")\n",
                "print(f\"üîë COPY THIS URL: {public_url}\")\n",
                "print(\"\\nüìã Add this to your Render Environment Variables:\")\n",
                "print(f\"   OLLAMA_BASE_URL={public_url}\")"
            ],
            "metadata": {
                "id": "tunnel"
            },
            "execution_count": null,
            "outputs": []
        }
    ]
}