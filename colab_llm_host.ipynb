{
    "nbformat": 4,
    "nbformat_minor": 0,
    "metadata": {
        "colab": {
            "provenance": [],
            "gpuType": "T4"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3"
        },
        "language_info": {
            "name": "python"
        }
    },
    "cells": [
        {
            "cell_type": "markdown",
            "source": [
                "# ðŸš€ Free GPU LLM Host (Ollama + Ngrok)\n",
                "Run this notebook to host `gemma2:2b` or `phi3.5` on a Google T4 GPU and expose it via API to your Render backend."
            ],
            "metadata": {
                "id": "header"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# 1. Install Ollama & Ngrok\n",
                "!curl -fsSL https://ollama.com/install.sh | sh\n",
                "!pip install pyngrok"
            ],
            "metadata": {
                "id": "install"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# 2. Configure Ngrok (REQUIRED)\n",
                "import getpass\n",
                "from pyngrok import ngrok, conf\n",
                "\n",
                "print(\"Enter your Ngrok Authtoken (from dashboard.ngrok.com):\")\n",
                "token = getpass.getpass()\n",
                "conf.get_default().auth_token = token"
            ],
            "metadata": {
                "id": "auth"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# 3. Start Server\n",
                "import os\n",
                "import threading\n",
                "import time\n",
                "import subprocess\n",
                "\n",
                "def start_ollama():\n",
                "    os.environ['OLLAMA_HOST'] = '0.0.0.0:11434'\n",
                "    os.environ['OLLAMA_ORIGINS'] = '*'\n",
                "    subprocess.run([\"ollama\", \"serve\"])\n",
                "\n",
                "threading.Thread(target=start_ollama, daemon=True).start()\n",
                "time.sleep(5)\n",
                "\n",
                "# Pull Model\n",
                "!ollama pull gemma2:2b"
            ],
            "metadata": {
                "id": "start"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# 4. Expose Public URL\n",
                "public_url = ngrok.connect(11434, bind_tls=True).public_url\n",
                "print(\"âœ… Universal LLM API Ready!\")\n",
                "print(f\"ðŸ”‘ COPY THIS URL: {public_url}\")"
            ],
            "metadata": {
                "id": "tunnel"
            },
            "execution_count": null,
            "outputs": []
        }
    ]
}