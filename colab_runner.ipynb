{
    "nbformat": 4,
    "nbformat_minor": 0,
    "metadata": {
        "colab": {
            "provenance": [],
            "gpuType": "T4"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3"
        },
        "language_info": {
            "name": "python"
        }
    },
    "cells": [
        {
            "cell_type": "markdown",
            "source": [
                "# üöÄ LPU Bot - Universal Runner (V8 - Deployment Ready)\n",
                "**Works on:** Google Colab Cloud (Linux) AND VS Code (Windows/Mac)."
            ],
            "metadata": {
                "id": "intro"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# 1. Setup Environment\n",
                "import platform, os, sys, subprocess\n",
                "IS_WINDOWS = (platform.system() == 'Windows')\n",
                "print(f\"üñ•Ô∏è OS Detected: {platform.system()}\")\n",
                "\n",
                "if IS_WINDOWS:\n",
                "    print(\"‚ÑπÔ∏è Windows Mode: Skipping Linux installation.\")\n",
                "else:\n",
                "    print(\"‚ÑπÔ∏è Linux Mode: Installing Ollama...\")\n",
                "    !curl -fsSL https://ollama.com/install.sh | sh"
            ],
            "metadata": {
                "id": "setup_env"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# 2. Start Ollama\n",
                "import time\n",
                "if not IS_WINDOWS:\n",
                "    print(\"‚è≥ Starting Ollama...\")\n",
                "    subprocess.Popen([\"ollama\", \"serve\"], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
                "    time.sleep(5)\n",
                "    print(\"‚úÖ Ollama Running\")\n",
                "else:\n",
                "    print(\"‚úÖ Assumed running on Windows.\")"
            ],
            "metadata": {
                "id": "start_ollama"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# 3. Pull Model\n",
                "print(\"‚¨áÔ∏è Pulling gemma2:2b...\")\n",
                "!ollama pull gemma2:2b"
            ],
            "metadata": {
                "id": "pull_model"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# 4. Unzip Project\n",
                "import os\n",
                "import zipfile\n",
                "\n",
                "if not os.path.exists('project.zip'):\n",
                "    print(\"‚ö†Ô∏è Project.zip NOT FOUND!\")\n",
                "    print(\"üëâ Uplodad 'project.zip' via the interface!\")\n",
                "else:\n",
                "    print(\"üì¶ Unzipping project...\")\n",
                "    with zipfile.ZipFile(\"project.zip\", 'r') as zip_ref:\n",
                "        zip_ref.extractall(\".\")\n",
                "    print(\"‚úÖ Unzip Done!\")\n",
                "\n",
                "if os.path.exists(\"src/llm_agent.py\"):\n",
                "    print(\"‚úÖ Code verified.\")\n",
                "else:\n",
                "    print(\"‚ùå ERROR: src/llm_agent.py missing. Zip might be empty/wrong.\")"
            ],
            "metadata": {
                "id": "unzip"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# 5. Install Dependencies\n",
                "!pip install -r requirements.txt"
            ],
            "metadata": {
                "id": "install_deps"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# 6. Ingest Data\n",
                "!python src/ingest.py"
            ],
            "metadata": {
                "id": "ingest"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# 7. Run Agent (Console Mode)\n",
                "import sys, os, asyncio, platform, subprocess\n",
                "import requests\n",
                "from contextlib import AsyncExitStack\n",
                "from mcp import ClientSession, StdioServerParameters\n",
                "from mcp.client.stdio import stdio_client\n",
                "\n",
                "# --- HEADER: PRE-FLIGHT CHECKS ---\n",
                "print(\"üîç Running Pre-Flight Checks...\")\n",
                "\n",
                "# 1. Check Source Code\n",
                "if not os.path.exists('src/llm_agent.py'):\n",
                "    raise FileNotFoundError(\"‚ùå CODE MISSING: src/llm_agent.py not found. Please run Step 4 (Unzip) again.\")\n",
                "\n",
                "# 2. Check Ollama Server\n",
                "try:\n",
                "    response = requests.get(\"http://127.0.0.1:11434\", timeout=2)\n",
                "    if response.status_code == 200:\n",
                "        print(\"‚úÖ Ollama Server is UP.\")\n",
                "    else:\n",
                "        print(f\"‚ö†Ô∏è Ollama responded with {response.status_code}\")\n",
                "except:\n",
                "    print(\"‚ùå OLLAMA DOWN: Connection refused.\")\n",
                "    print(\"üëâ FIX: Please Run Step 2 (Start Ollama) again. It stops when Colab restarts.\")\n",
                "    raise ConnectionError(\"Ollama not running.\")\n",
                "\n",
                "# 3. Check Model\n",
                "try:\n",
                "    m_resp = requests.get(\"http://127.0.0.1:11434/api/tags\")\n",
                "    models = [m['name'] for m in m_resp.json()['models']]\n",
                "    if \"gemma2:2b\" not in models:\n",
                "        print(f\"‚ö†Ô∏è WARNING: gemma2:2b not found in {models}\")\n",
                "        print(\"üëâ FIX: Run Step 3 (Pull Model) again.\")\n",
                "    else:\n",
                "        print(\"‚úÖ Model 'gemma2:2b' is ready.\")\n",
                "except:\n",
                "    pass\n",
                "\n",
                "# --- END CHECKS ---\n",
                "\n",
                "if platform.system() == 'Windows':\n",
                "    try: asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy())\n",
                "    except: pass\n",
                "\n",
                "if 'src' not in sys.path: sys.path.append(os.path.abspath('src'))\n",
                "\n",
                "class SafeUniMcpClient:\n",
                "    def __init__(self):\n",
                "        self.session = None\n",
                "        self.exit_stack = AsyncExitStack()\n",
                "    async def __aenter__(self):\n",
                "        params = StdioServerParameters(\n",
                "            command=sys.executable,\n",
                "            args=[os.path.abspath(\"src/mcp_server.py\")],\n",
                "            env=os.environ.copy()\n",
                "        )\n",
                "        try:\n",
                "            read, write = await self.exit_stack.enter_async_context(stdio_client(params, errlog=subprocess.DEVNULL))\n",
                "            self.session = await self.exit_stack.enter_async_context(ClientSession(read, write))\n",
                "            await self.session.initialize()\n",
                "            return self\n",
                "        except Exception as e:\n",
                "            await self.exit_stack.aclose()\n",
                "            raise e\n",
                "    async def __aexit__(self, exc_type, exc_val, exc_tb): await self.exit_stack.aclose()\n",
                "    async def get_tools(self): return (await self.session.list_tools()).tools\n",
                "    async def call_tool(self, name, args): return await self.session.call_tool(name, arguments=args)\n",
                "\n",
                "import llm_agent\n",
                "import importlib\n",
                "importlib.reload(llm_agent)\n",
                "llm_agent.UniMcpClient = SafeUniMcpClient\n",
                "agent = llm_agent.UniAgent()\n",
                "print(\"ü§ñ Bot active (Console Mode). Type message below.\")\n",
                "await agent.chat_loop()"
            ],
            "metadata": {
                "id": "run_agent"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# 8. Launch UI (Gradio)\n",
                "# This will give you a PUBLIC URL (e.g. https://1234.gradio.live)\n",
                "!python web_app.py"
            ],
            "metadata": {
                "id": "run_ui"
            },
            "execution_count": null,
            "outputs": []
        }
    ]
}